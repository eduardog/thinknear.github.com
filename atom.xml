<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Thinknear Engineering]]></title>
  <link href="http://engineering.thinknear.com/atom.xml" rel="self"/>
  <link href="http://engineering.thinknear.com/"/>
  <updated>2015-04-24T09:46:51-07:00</updated>
  <id>http://engineering.thinknear.com/</id>
  <author>
    <name><![CDATA[Thinknear by Telenav]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Testing @ Thinknear - Part One: Technology Choices]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/04/23/testing-at-thinknear-part-one-technology-choices/"/>
    <updated>2015-04-23T12:50:04-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/04/23/testing-at-thinknear-part-one-technology-choices</id>
    <content type="html"><![CDATA[<p>At Thinknear we believe automated tests are essential. Having strong automated test coverage allows the team to:</p>

<ul>
<li>deploy production code faster, while maintaining quality</li>
<li>make bolder changes to the system with confidence</li>
<li>increase confidence that a certain change won&rsquo;t break important features of the systems</li>
<li>decrease TCO for the entire product lifecycle</li>
<li>analyze how the product would scale up/out, through PRS (Performance, Reliability, Stress) tests</li>
</ul>


<p>Thinknear&rsquo;s systems are based around two main development languages, Ruby and Java. Java is used for the real-time, high frequency bidding engine, while Ruby (on Rails)
is used elsewhere.</p>

<p>We test at the unit level, service level and integration level (end-to-end) using a veriety of tools:</p>

<ul>
<li>for the Ruby on Rails systems: RSpec, Capybara, Poltergeist, Karma</li>
<li>for the Java-based systems: JUnit, Hamcrest, Sonar</li>
<li>for the build management system: Jenkins</li>
</ul>


<p>Those tools were chosen in correlation with those development languages, the only exception being that we chose to write the service level tests
for our Java-based systems in RSpec.</p>

<ol>
<li><p>Why <strong>RSpec</strong>? The layout of the tests focuses on how the software should behave (BDD) and follows natural language in a very close manner, the vocabulary and
structure of the tests being more intuitive than we found in other testing frameworks. An example is the matchers, supporting both positive and negative expectations.</p></li>
<li><p>Why <strong>Capybara</strong>? Capybara is used for UI browser testing, it allows easy switching of drivers, provides an efficient DSL for interacting with the DOM. We also
evaluated Selenium, but chose Capybara because we found it to allow faster test development.
Writing 99% reliable code still comes with experience, ways to get to that percentage will be discussed in another post.</p></li>
<li><p>Why <strong>Poltergeist</strong>? Driver for PhantomJS (headless browser built on WebKit), the main reason for using it for our Capybara tests is speed. In our experiments,
it proved 4-10 times faster than Selenium&rsquo;s full browser driver. Poltergeist has very clear error messages and it can re-raise Javascript errors from the page.</p></li>
<li><p>Why <strong>Karma</strong>? Our UI relies heavily on AngularJS, therefore Karma, built by the AngularJS team, is the default solution for testing Javascript at the unit test level.
It supports many plugins, and lets you test in a cross-browser environment.</p></li>
<li><p>Why <strong>JUnit</strong>? The standard for testing in Java, JUnit is very well documented, supported by almost any IDE, is used for unit level testing,
developers writing test cases while developing the software. TestNG is also a popular framework, we went with JUnit since this is what the team was most experienced with.</p></li>
<li><p>Why <strong>Hamcrest</strong>? A library of matcher objects, makes the code more readable by allowing match rules to be defined declaratively (closer to natural language).
We use it in combination with JUnit for our Java-based software.</p></li>
<li><p>Why <strong>Sonar</strong>? Sonar provides a plugin architecture for different analysis tools like FindBugs, Squid etc. It can aggregate information from those analyzers,
including those for understanding unit test code coverage. It has dashboards and historical views across all projects, and works well with the Jenkins build pipeline.
Sonar can also analyze other languages, although that feature can be immature depending on the language. Besides code coverage, we use it to detect package cycles
(circular dependencies) and to analyze code complexity.</p></li>
<li><p>Why <strong>Jenkins</strong>? Popular continuous build tool, free and open source, Jenkins supports a wide array of plugins and features like support for SCM changes, console logs,
email notifications, build exclusions and many others. Its flexibility allows it to fit in a variety of environments and makes it easy to develop complicated workflows.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Migration and Dependencies on User Defined Tables/Views]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/03/30/rails-migration-and-dependencies-on-user-defined-tables-slash-views/"/>
    <updated>2015-03-30T10:47:39-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/03/30/rails-migration-and-dependencies-on-user-defined-tables-slash-views</id>
    <content type="html"><![CDATA[<p>Thinknear Engineering values reporting because it allows us to monitor how our mobile advertising platform behaves in its <a href="http://en.wikipedia.org/wiki/Real-time_bidding">real time bidding</a> environment.
We also value business intelligence, because it gives Thinknear&rsquo;s operations and data science teams a tool to perform analytics and fine tune our marketing platform to ensure it meets our customers&#8217; needs and exceeds their expectations.</p>

<p>To this purpose, we are using <a href="http://aws.amazon.com/redshift/">AWS&#8217; Amazon Redshift</a> as the main tool of our BI stack and we handle data loads and migrations through our <a href="http://rubyonrails.org/">Ruby on Rails</a> applications.
Being a BI tool, our redshift clusters need to accommodate tables and views created not only by our applications but by our operations and data science teams, as well.
It is quite common for user defined tables and views to rely on application defined tables and views, which makes migrations a challenge.</p>

<p>In the following post, we present two SQL queries that are useful when trying to identify dependencies before running migrations.</p>

<h2>Find locks held by running queries</h2>

<p>Queries ran by users can hold locks on tables that the migration might operate on.
If a user query holds a lock on a table that is bound to change by a migration, the migration will hang until the user query finishes (which in our case can sometimes be several hours).
Empirically, we have found that querying the catalog view <a href="http://www.postgresql.org/docs/9.4/static/view-pg-locks.html">PG_LOCKS</a> for finding existing locks tends to be more accurate than querying the <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOCKS.html">STV_LOCKS</a> table.
Based on that, the following query returns all the locks on database objects (in any schema) in a redshift cluster.</p>

<pre><code>SELECT getdate(),
c.relname,
n.nspname,
l.database,
l.transaction,
l.pid,
a.usename,
l.mode,
l.granted
FROM pg_catalog.pg_locks l
JOIN pg_catalog.pg_class c ON c.oid = l.relation
JOIN  pg_namespace n ON n.oid = c.relnamespace
JOIN pg_catalog.pg_stat_activity a ON a.procpid = l.pid;
</code></pre>

<p>In the query above:</p>

<ol>
<li><code>getdate()</code> is the current timestamp. <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_GETDATE.html">See GETDATE documentation</a></li>
<li><code>c.relname</code> is the name of the table or view the lock is held on.</li>
<li><code>n.nspname</code> is the name of the schema that the table or view belongs to.</li>
<li><code>l.database</code> is the current database id or a special value as described in <a href="http://www.postgresql.org/docs/9.4/static/view-pg-locks.html">PG_LOCKS table documentation</a>.</li>
<li><code>l.transaction</code> is the transaction the query belongs to.</li>
<li><code>l.pid</code> is the pid of the query that obtained the lock.</li>
<li><code>a.usename</code> is the name of the user that issued the query that holds the lock.</li>
<li><code>l.mode</code> is the lock type (e.g. AccessShareLock).</li>
<li><code>l.granted</code> is true or false if the lock was granted or not.</li>
</ol>


<p>This query is slightly better than the one provided in the <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOCKS.html">STV_LOCKS example</a> since it will give the username and table or view name out of the box.
As a result, the engineer who runs the migration knows which table or view might be an impediment and they would also know who to talk to, before running the migration.</p>

<h2>Find dependencies for a specific column</h2>

<p>One type of potential dependency between user defined objects and application defined ones could be a <em>foreign key</em> constraint.
This type of constraint can be found using the query mentioned <a href="http://stackoverflow.com/a/1152321">here</a>.
However, there are different types of dependencies apart from <em>foreign key</em> constraints.</p>

<p>An example of such direct dependency is a user defined view that is built up by columns of application defined table(s).
Imagine there is a table called <code>orders</code> that is partitioned monthly in the redshift database.
Now, let us assume that the operations team wants to create views that gather all orders to produce quarterly reports.
As a result, one of these quarterly views might be defined by the operations team like this:</p>

<pre><code>CREATE VIEW orders_2015_Q1 AS 
    SELECT column_name1, column_name2... from orders_201501 UNION ALL
    SELECT column_name1, column_name2... from orders_201502 UNION ALL
    SELECT column_name1, column_name2... from orders_201503 UNION ALL
    SELECT column_name1, column_name2... from orders_201504;
</code></pre>

<p>This view does not have a foreign key constraint to any of the four partitions of the orders table it uses.
However, it has a dependency on the columns of those partitions.
If a migration wants to drop or alter the type of any of columns <code>column_name1,column_name2...</code>, the migration will fail.
In the case of a drop column, a <code>cascade</code> clause can be used to drop the column to all dependent objects.
Since the tables or views that reference the altered columns might be user defined, we need to ensure it is safe to use the <code>cascade</code> clause before running the migration.</p>

<p>The following query can be used to spot dependencies like the one explained above:</p>

<pre><code>SELECT distinct dependee.relname   
FROM pg_depend   
JOIN pg_rewrite ON pg_depend.objid = pg_rewrite.oid   
JOIN pg_class as dependee ON pg_rewrite.ev_class = dependee.oid   
JOIN pg_class as dependent ON pg_depend.refobjid = dependent.oid   
JOIN pg_attribute ON pg_depend.refobjid = pg_attribute.attrelid   
AND pg_depend.refobjsubid = pg_attribute.attnum   
WHERE dependent.relname = 'orders_201503'  
AND pg_attribute.attnum &gt; 0   
AND pg_attribute.attname = 'column_name1';   
</code></pre>

<p>The query above will find any database objects (tables, views) that use column <code>column_name1</code> from table <code>orders_201503</code>.</p>

<p>Alternatively to using a <code>cascade</code> clause, one might want to drop the <code>orders_2015_Q1</code> view and restore it after the migration has finished.
The definition of a view can be found before the migration:</p>

<pre><code>SELECT definition FROM pg_views WHERE viewname='orders_2015_Q1';
</code></pre>

<p>After the migration, the view can be restored using the new types of columns or omitting columns that were dropped.</p>

<h2>Handle everything in the application</h2>

<p>Obviously, the best approach is to strive for having only application code handle the schema definition in your BI database.
If operations and data science are heavily using a view or table created outside of the application, then that view or table needs to be part of the application defined schema.
However, there are cases where that is not efficient, since SQL proficient data scientists can quickly write SQL code to perform their analysis without having to request the product development team for a migration.
In the end, it&rsquo;s a trade-off that the engineering team needs to measure, but if the schema can be manipulated by both users and the application, then the queries presented above can help identify potential migration problems in advance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interviewing With Thinknear]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/03/09/interviewing-with-thinknear/"/>
    <updated>2015-03-09T12:35:22-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/03/09/interviewing-with-thinknear</id>
    <content type="html"><![CDATA[<p>We are hiring like crazy here at Thinknear.
(Current openings on our <a href="http://www.thinknear.com/career/">careers page</a>.)
We&rsquo;re solving massive scale challenges in the hundreds of thousands of requests per second, pressing databases to the limit, and we have more data than we know what to do with.
As a result, we&rsquo;re looking for engineers, data scientists, and managers.
All these people are going through our process, and I wanted to share a bit more about that.</p>

<p>I&rsquo;m going to skip the part where candidates first hear about us.
We connect with people through recruiters, linked in, personal networks, events hosted at our offices, on campuses, &hellip;. so it&rsquo;s different for everyone.</p>

<p>The first step in our process for junior and mid-level candidates is to complete a coding challenge.
This is an online coding test, the goal of which is to provide a coding sample to an engineer to review before doing a phone screen.
This step exists because we get a lot of people applying for positions who can&rsquo;t code.
It&rsquo;s a bit frustrating, but this step helps save time for both parties &ndash; since the position here requires writing lots of code, we really want to make sure up front that we cover it.</p>

<p>With Sr. candidates we will often opt for what we call a &lsquo;sales call&rsquo;.
This is a call between the candidate and a people manager where we discuss what the candidate is looking for and attempt to determine if Thinknear will be the right fit.
Like coding challenges, this is intended to save both the time of our engineers as well as that of the candidate - lets not spend time going through the interview if we don&rsquo;t think there&rsquo;s a good fit.</p>

<p>The next step is a technical phone screen.
This is a call with an engineer here at Thinknear with whom you will solve a problem using code.
To do this, we ask candidates to share a Google doc with the interviewer, and then solve a problem (without the aid of an IDE).</p>

<p>We like &lsquo;simple&rsquo; problems which are solvable by applying data structures and algorithms.
We don&rsquo;t like &lsquo;trick&rsquo; problems (problems that have 1 non-obvious solution which is easy if you&rsquo;ve seen it before but hard if you haven&rsquo;t).
Simple problems involve sorting arrays, traversing trees, hashing strings, counting, and other problems you might encounter in a CS undergrad education.
Our problems generally have multiple solutions, and though we&rsquo;d like to see you produce the optimal solution, that&rsquo;s not necessarily the primary evaluation criteria.
We&rsquo;re really looking for how you approach the problem, the quality of the code you produce, how you verify your solution is correct, and how you analyze it&rsquo;s performance.</p>

<p>We vary between 1 and 2 phone screens.
After that, we ask candidates to come meet us in person.
We will pay for travel for non-local candidates (and if later make an offer, we also cover relocation).</p>

<p>For the in house, we ask that candidates wear comfortable clothes akin to what they&rsquo;d wear to work (except for Halloween &ndash; seriously, it&rsquo;ll be weird if you wear a costume).
You won&rsquo;t wear a suit here on a regular basis, there&rsquo;s no point wearing one to your interview.</p>

<p>The in house interviews vary a lot depending on the position and level.
Junior engineering candidates will tend to meet with more engineers and their interviews will focus very strongly on coding.
Mid-level engineers will have product folks added to the loop and spend some time on design.
Senior engineers will do more design and talk more about leadership, team building, and architecture.
Management types will get management type questions (e.g. working with underperformers).
We&rsquo;ll usually do lunch or a break of some sort, which will give you a chance to socialize with the potential team.</p>

<p>Please bring your questions to the in house!
We love it when candidates have good questions for us, and we like people who are more discerning in their selection.
Remember, the in house is as much a chance for you to interview us as it is for us to interview you.
By the end of the day, you should have a very good idea of what it might be like to work here, and if we make an offer, we really hope you will.</p>

<p>We&rsquo;re very picky with who we hire, but that means that if you make it all the way through, you get to work with really great engineers.
For many people, that&rsquo;s incentive enough.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding the Decision to Move From AWS EMR/Hive to Redshift]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/01/21/understanding-the-decision-to-move-from-aws-emr-slash-hive-to-redshift/"/>
    <updated>2015-01-21T17:40:47-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/01/21/understanding-the-decision-to-move-from-aws-emr-slash-hive-to-redshift</id>
    <content type="html"><![CDATA[<p>At Thinknear we always want to make sure we are doing our best to use the right tool for the job. So when Redshift came out we decided to evaluate our current reporting and analytics pipeline and see if Redshift could help us improve. At the time we were using Hive/Hadoop on EMR for all our reporting and analytics purposes. We saw Redshift as a way to speed up our reporting infrastructure without completely rearchitecting and give our business team a much easier way to access the data. Given these goals we evaluated Redshift against our current Hive/Hadoop solution and found the following pros and cons.</p>

<p>Pros:</p>

<ol>
<li>Redshift is fast.

<ul>
<li>For a lot of our critical reporting jobs, once the data was loaded, jobs that were taking 10-30 minutes were down to the 1-2 minute range.</li>
</ul>
</li>
<li>Ad-hoc analysis is easier.

<ul>
<li>The learning curve to use redshift is much less than hive/hadoop. For our business and data analysts it was often extra overhead to explain and understand hadoop clusters worked.</li>
<li>Redshift has also been much less surprising so far. Hive has some limitations with more advanced queries that we have not seen with Redshift.</li>
</ul>
</li>
</ol>


<!-- more -->


<ol>
<li>No longer dealing with EMR/Hadoop versions

<ul>
<li>A lot of issues we had with EMR was with new versions of hadoop/EMR. Things would break with bad builds or different performance characteristics and they were often a pain to deal with. It seems with Redshift more of this is abstracted away and we haven’t had to deal with any of these types of issues.</li>
</ul>
</li>
<li>Cost

<ul>
<li>For a lot of our Hive tasks we had to roll up our data in various ways (hourly, daily, weekly, etc.), before we could use it in our reports. This caused us to have a lot of clusters and was getting expensive. With redshift we can sometimes skip roll ups and the jobs are running much faster. It turned out that the costs were less for Redshift than hive for some of our reporting pipeline.</li>
</ul>
</li>
</ol>


<p>Cons:</p>

<ol>
<li>Redshift does not easily scale up for dynamic workloads.

<ul>
<li>The reporting pipeline at thinknear can fluctuate for large advertising days. With EMR we can spin up extra worker nodes to scale up easily. If Redshift becomes too slow or backed up due to high data or query volumes, adding nodes is very time consuming. Resizing is the most straight forward, but puts your cluster into read only mode for an extended period, which we can’t live with. This means that snapshotting and reloading our data is the only approach and that can be a multi day process for us.</li>
</ul>
</li>
<li>Redshifts workload management does not fully solve the parallel job problem.

<ul>
<li>With EMR, running jobs in parallel is pretty safe, one job is not going to affect another job. Redshift has workload management which allows you to split workers into queues. These queues have limits on the amount of memory they can use but not the amount of CPU. A lot of our jobs are CPU intensive so this limits the amount of jobs we are able to run in parallel.</li>
<li>Redshift is also limited by disk space for some queries. When running multiple jobs with big joins you may run into disk space limitations. To be fair this may be solvable by writing more efficient queries.</li>
</ul>
</li>
<li>Loading data into your Redshift cluster takes time.

<ul>
<li>We have some jobs that run hourly and so we need data loaded into our cluster hourly. If we want to maintain acceptable performance we need to vacuum our data (Sidenote: Amazon docs say that you may not need to vacuum your data if you load it in sequential order, we saw performance improvements when we ran the Vacuum anyway). For us the overall time to load and run the queries were still faster than running Hive but it is still something to take into consideration.</li>
</ul>
</li>
<li>With Redshift you have to define schemas with lengths and encodings.

<ul>
<li>In EMR you do not need to have encodings and lengths defined when dealing with data. You just define what it looks like in terms of data types and hive handles the rest. Redshift needs the encoding and lengths to be more efficient to be more efficient on disk space. This means you have some upfront costs to figure these things out before importing a table into redshift.</li>
</ul>
</li>
<li>Redshift does not offer any custom function or array support.

<ul>
<li>We didn’t use a lot of custom functions in Hive (UDF/UDAF), but the ones we did use were very useful. Some of the data we wanted to analyze used arrays so in order to get it into redshift it required some pre-processing or ugly queries.</li>
</ul>
</li>
<li>Redshift has a higher cost of failure

<ul>
<li>This is one we didn’t really think about until it happened to us. One of our clusters had a failure and they had to replace a node, causing a complete rebalance of data. This slows performance for an extended period of time (depending on how much data you have) and can become a major issue if you have critical processes running in Redshift. EMR would fail as well but the cost of booting up another cluster and running it was much less.</li>
</ul>
</li>
</ol>


<p>In the end we moved the processes that made the most sense to Redshift and left some on EMR. The real win was for our analytics team, we ended up moving a lot of our data onto a separate redshift cluster for them. They previously had some visual tools and would use Hive to dig deeper. The ease of using redshift really made them more productive and in turn allowed us to get rid of other services in favor of Redshift.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Angular UI Router Part I: Nested Views]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/01/07/advanced-angular-ui-router-part-i/"/>
    <updated>2015-01-07T13:18:03-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/01/07/advanced-angular-ui-router-part-i</id>
    <content type="html"><![CDATA[<p>At ThinkNear, we have an in-house administrative dashboard that our ad operations team uses to set up and manage ad campaigns.
The dashboard is an AngularJS frontend with a Ruby on Rails backend, with the
 <a href="https://github.com/angular-ui/ui-router">ui-router</a> plugin for permalinks and navigation.
While ngNewsletter&rsquo;s <a href="http://www.ng-newsletter.com/posts/angular-ui-router.html">Diving deep into the AngularUI Router</a> was a helpful primer, while creating the system we found it didn&rsquo;t go deep enough.
This post describes how we used ui-router&rsquo;s Nested Views feature to achieve the site layout we wanted.</p>

<!-- more -->


<h3>Background</h3>

<p>Our object hierarchy goes Contract > Insertion Order > Campaign.
We have many Contracts, Contracts can have many Insertion Orders, and Insertion Orders can have many Campaigns.
Ad Ops often needed to reference one object while working on another.
They preferred to open multiple tabs or browser windows and look at them side-by-side.
They need an easy way to open multiple Contract tabs from the main list, and a good single-Contract view.
They wanted to be able to use the back button to go up the hierarchy.
And they wanted to be able to pass around short links to specific objects that loaded quickly.</p>

<p>Here is the page structure we came up with:</p>

<p>Contract List View:</p>

<pre><code>[Site Header]
  [Contract Section Header]
    [Contract1]
    [Contract2]
    …
[Site Footer]
</code></pre>

<p>Contract Detail View:</p>

<pre><code>[Site Header]
  [Contract Section Header]
    [Contract 1 Header]
      [Back to List button]
    [Contract 1 Details]
      [Contract 1 Insertion Order 1]
      [Contract 1 Insertion Order 2]
        …
[Site Footer]
</code></pre>

<p>Insertion Order Detail View:</p>

<pre><code>[Site Header]
  [Contract Section Header]
    [Insertion Order 1 Header]
      [Back to Contract button]
    [Insertion Order 1 Details]
      [Insertion Order 1 Campaign 1]
      [Insertion Order 1 Campaign 1]
      …
[Site Footer]
</code></pre>

<p>Campaign Detail View:</p>

<pre><code>[Site Header]
  [Contract Section Header]
    [Campaign 1 Header]
      [Back to Insertion Order button]
    [Campaign 1 Details]
[Site Footer]
</code></pre>

<p>And here&rsquo;s the URL structure for permalinks:</p>

<ul>
<li>/contracts - List</li>
<li>/contracts/new - New contract</li>
<li>/contracts/1 - Contract 1 details</li>
<li>/contracts/insertion_orders/new - New insertion order</li>
<li>/contracts/insertion_orders/1 - Insertion order 1 details</li>
<li>/contracts/campaigns/new - New campaign</li>
<li>/contracts/campaigns/1 - Campaign 1 details</li>
</ul>


<p>Once states are set up, UI Router handles permalinks and back button support.
The tricky part was organizing the states to get both the permalinks and site layout we wanted.
The flat permalinks were important not only for URL length, but to improve load speed by not requesting all of an object&rsquo;s parents.</p>

<p>We tried nesting the states but using absolute URL patterns to flatten the URL structure:</p>

<pre><code>.state('contracts', { 
  url: "/contracts" 
})
.state('contracts.contract_selected', { 
  url: "/{contract_id:[0-9]+}"
})
.state('contracts.contract_selected.insertion_order_selected', { 
  url: "^/contracts/insertion_orders/{insertion_order_id:[0-9]+}"
  // Absolute URL with no contract ID
})
</code></pre>

<p>Unfortunately, this doesn&rsquo;t work because UI router requires the <code>contract_id</code> be present for the parent state!
So we structured the states to mirror our permalink structure:</p>

<pre><code>.state('contracts', { 
  url: "/contracts" 
})
.state('contracts.contract_selected', { 
  url: "/{contract_id:[0-9]+}"
})
.state('contracts.insertion_order_selected', { 
  url: "/insertion_orders/{insertion_order_id:[0-9]+}"
})
</code></pre>

<p>To get the site layout we wanted, we took advantage of UI Router&rsquo;s <a href="https://github.com/angular-ui/ui-router/wiki/Multiple-Named-Views">named views</a>.
Named views allow you to have DOM elements from child states replace DOM elements from parents.
If you declare a state with the <code>templateUrl</code> and <code>controller</code> at the top level,
it will implicitly insert it in an un-named <code>ui-view</code> element in the parent.
But if you declare a <code>views</code> property in your state, you can name views, and specify where in the hierarchy they should be inserted.</p>

<h3>Ui-Router Templates and States</h3>

<p>Here&rsquo;s the final structure of states and templates:</p>

<h5>Main Site Template</h5>

<p>This was a static template, no dynamic content.</p>

<pre><code>&lt;div class="header"&gt;Site Header&lt;/div&gt;
&lt;div ui-view&gt;&lt;/div&gt;
&lt;div class="footer"&gt;Footer&lt;/div&gt;
</code></pre>

<h4>Contract Section</h4>

<h5>State</h5>

<pre><code>.state('contracts', {
  url: "/contracts",
  abstract: true,
  views: {
    "": {
      controller: 'ContractSectionController',
      templateUrl: 'contracts/index.html'
    },
  },
})
</code></pre>

<h5>Template</h5>

<pre><code>&lt;div class="header"&gt;Contract Section Header&lt;/div&gt;
&lt;div ui-view="main"&gt;&lt;/div&gt;
</code></pre>

<h4>Contract List</h4>

<p>This is the state that shows a list of all contracts.</p>

<h5>State</h5>

<p>When a state&rsquo;s parent is abstract, setting the URL matcher to an empty string means that it will automatically go to this child state when you enter the URL for the parent.
Set up like this, going to <code>http://root.com/#/contracts</code> will take you to the contracts.list state:</p>

<pre><code>.state('contracts.list', {
    url: "",
    views: {
        "main@contracts": {
            controller: 'ContractListController',
            templateUrl: 'contracts/list.html'
        }
    }
})
</code></pre>

<h5>Template</h5>

<pre><code>&lt;div class="list"&gt;
    &lt;div ng-repeat="item in list"&gt;
        &lt;!-- list item content --&gt;
    &lt;/div&gt;
    &lt;div class="pagination"&gt;&lt;/div&gt;
&lt;/div&gt;
</code></pre>

<h4>Contract Details</h4>

<p>This is the state that shows a single contract.
It also functions as the permalink for that contract.</p>

<h5>Single Item Template</h5>

<p>Our single-item views had common elements, so we extracted those into a details template.</p>

<pre><code>&lt;div class="detail-header"&gt;Item Header Content&lt;/div&gt;
&lt;div ui-view="details"&gt;&lt;/div&gt;
</code></pre>

<h5>Contract Detail Template</h5>

<p>This is shared by the existing item and new item states.</p>

<pre><code>&lt;div class="contract"&gt;
    &lt;!-- contract stuff goes here --&gt;
&lt;/div&gt;
</code></pre>

<h5>Contract Detail and New Contract State</h5>

<p>Here you can see named views in action.
<code>main@contracts</code> inserts the Single Item template into <code>&lt;div ui-view="main"&gt;</code>, replacing the list.
Then <code>details@contracts.contract_selected</code> inserts the Contract Detail template into <code>&lt;div ui-view="details"&gt;</code>.
The url matcher for <code>contract_selected</code> uses UI Router&rsquo;s <a href="https://github.com/angular-ui/ui-router/wiki/URL-Routing#basic-parameters">curly brace syntax</a> to specify a regex for <code>contract_id</code>, to prevent it from matching on the <code>/new</code> we use for new contracts.</p>

<pre><code>.state('contracts.contract_selected', {
    url: "/{contract_id:[0-9]+}",
    views: {
        "main@contracts": {
            controller: 'SingleItemController',
            templateUrl: 'contracts/single_item.html'
        },
        'details@contracts.contract_.selected', {
            controller: 'ContractDetailController',
            templateUrl: 'contracts/detail.html
        }
    }
})
</code></pre>

<p>The new contract state is almost identical, but since we don&rsquo;t have an ID until the contract is saved, it matches on <code>/new</code>.</p>

<pre><code>.state('contracts.contract_new', {
    url: "/new",
    …
})
</code></pre>

<h4>Insertion Order and Campaign Details</h4>

<p>The Insertion Order and Campaign Detail states look just like Contract Detail State.</p>

<pre><code>.state('contracts.insertion_order_selected', {
    url: "/{insertion_order_id:[0-9]+}",
    views: {
        "main@contracts": {
            controller: 'SingleItemController',
            templateUrl: 'contracts/single_item.html'
        },
        'details@contracts.insertion_order_selected', {
            controller: 'InsertionOrderDetailController',
            templateUrl: 'insertion_orders/detail.html
        }
    }
})
.state('contracts.campaign_selected', {
    url: "/{campaign_id:[0-9]+}",
    views: {
        "main@contracts": {
            controller: 'SingleItemController',
            templateUrl: 'contracts/single_item.html'
        },
        'details@contracts.campaign_selected', {
            controller: 'CampaignDetailController',
            templateUrl: 'campaigns/detail.html
        }
    }
})
</code></pre>

<h3>Conclusion and Next Steps</h3>

<p>So that&rsquo;s how to use UI Router to display drill-down views for nested models.
Key takeaways:</p>

<ul>
<li>If you use state parameters, your state structure will need to mirror your permalink structure.</li>
<li>Use named views to replace content from a parent state.</li>
</ul>


<p>You may still have some questions, such as:</p>

<ul>
<li>How do you get the data into the controllers?</li>
<li>Why isn&rsquo;t <code>contracts/single_item.html</code> part of an abstract parent state that the other detail views inherit from?</li>
</ul>


<p>Those will be answered in the next part, UI Router Resolves.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Heroku Cost Optimization Using Generic Workers]]></title>
    <link href="http://engineering.thinknear.com/blog/2015/01/06/heroku-cost-savings-using-generic-workers/"/>
    <updated>2015-01-06T17:43:10-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2015/01/06/heroku-cost-savings-using-generic-workers</id>
    <content type="html"><![CDATA[<p>During the early days of Thinknear, <a href="https://github.com/resque/resque" target="_blank">Resque</a> was the most prevalent background job processor for our Rails applications.
However, Resque was not multithread-friendly, and, as our applications grew, this put a toll on our Heroku monthly bill.</p>

<p>We tried out <a href="http://sidekiq.org" target="_blank">Sidekiq</a>, really liked it, and now use it exclusively. Unlike Resque, Sidekiq supports multiple threads
working on multiple jobs concurrently.
According to Sidekiq&rsquo;s main developer, Mike Perham, one large Resque farm with a 68GB RAM footprint can be brought down to 1 GB RAM by using threads instead of processes.
[<a href="https://github.com/mperham/sidekiq/wiki/Internals" target="_blank"><em>source</em></a>]</p>

<!-- more -->


<p>We analyzed the distribution of background jobs across dynos.
We use a family of dynos for each queue, running up to 50 dynos per queue and different families being on for 750 hours per month at the low end, and up to to 40,000 hours
per month at the high end.</p>

<p>We utilize a home-grown type of autoscaling to dial up and down workers based on the jobs in the queue, since all dyno families needing it are background job focused.
Its simplicity provided long-term reliability.</p>

<p>While this setup fit the production processing needs, it was inefficient for non-production environments.
With 19 always on dyno families, and 3 non-production environments (sandbox, test, and integration) used 24/7 for unit, contract level and end-to-end integration testing,
something needed to change.</p>

<p>Sidekiq supports assigning multiple queues to a single worker process. In non-production environments, we assigned every queue to a single, generic worker and
noticed a couple of standard dynos are enough to process the non-production load.</p>

<p>As a result, we&rsquo;re now saving 90-95% on the Heroku bill of development and testing environments.</p>

<p>Sample Procfile:</p>

<p><code>web: bundle exec unicorn -p $PORT -c ./config/unicorn.rb</code><br/>
<code>clock: bundle exec clockwork config/clock.rb</code></p>

<p><code>core_worker: bundle exec sidekiq -c 1 -q core</code><br/>
<code>creator_worker: bundle exec sidekiq -c 1 -q creator</code><br/>
<code>emr_worker: bundle exec sidekiq -c 1 -q emr</code><br/>
<code>maintenance_worker: bundle exec sidekiq -c 1 -q maintenance</code><br/>
<code>measure_worker: bundle exec sidekiq -c 1 -q measure</code></p>

<p><code>generic_worker: bundle exec sidekiq -c 1 -q core -q creator -q emr -q maintenance -q measure</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presenting at re:Invent]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/11/25/presenting-at-reinvent/"/>
    <updated>2014-11-25T16:07:43-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/11/25/presenting-at-reinvent</id>
    <content type="html"><![CDATA[<p>Thinknear was priviledged to be invited to present at AWS re:Invent 2014. Our topic was on how we have scaled to billions of daily requests on Elastic Beanstalk. Here&rsquo;s a direct link to our agenda summary, <a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=8726">APP 402</a>.</p>

<p>You can check us out on the <a href="http://bit.ly/1qR0mEi">AWS YouTube Channel</a>. You can view our slides on <a href="http://www.slideshare.net/AmazonWebServices/app402?qid=0a3f4642-757a-484b-a850-c34b0cb8b865&amp;v=default&amp;b=&amp;from_search=1">Slideshare</a>. You can even listen to us as a podcast on <a href="https://itunes.apple.com/us/podcast/app402-serving-billions-web/id941313180?i=324951733&amp;mt=2">iTunes</a>.</p>

<p>Thank you, AWS, for inviting us.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aws_templates Released]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/11/11/aws-templates-released/"/>
    <updated>2014-11-11T14:45:14-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/11/11/aws-templates-released</id>
    <content type="html"><![CDATA[<h2>Announcement</h2>

<p>Thinknear is releasing our <code>aws_templates</code> as an open-source project under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">APLv2</a>.
<code>aws_templates</code> is an example deployment and configuration setup for <a href="http://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>.</p>

<p>You can visit the project&rsquo;s <a href="https://github.com/ThinkNear/aws_templates">GitHub page</a>. The README has more information on the features and behavior of the configuration.</p>

<h2>Motivation</h2>

<p>Thinknear&rsquo;s Elastic Beanstalk environments are low-latency, high-throughput systems serving billions of auction requests per day from dozens of exchanges.
The <code>aws_templates</code> works with Beanstalk&rsquo;s <code>.ebextensions</code> to configure EC2 instances with</p>

<ul>
<li>RAID 0</li>
<li>log rotation to AWS S3 using <a href="https://github.com/ThinkNear/tn_s3_file_uploader">tn_s3_file_uploader</a></li>
<li>collectd and JMX remote monitoring</li>
<li>AWS ELB configuration</li>
<li>Custom system settings</li>
</ul>


<p>This project is intended to share a method of deployment and configuration using a collection of scripts and Beanstalk&rsquo;s <code>.ebextensions</code>.</p>

<h2>Contribute</h2>

<p>Thinknear Engineering would like to invite anyone who finds <code>aws_templates</code> useful to contribute to the project. Visit the project&rsquo;s <a href="https://github.com/ThinkNear/aws_templates/issues">issue tracker page</a> to submit questions, bugs, or new features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tn_s3_file_uploader Released]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/11/11/tn-s3-file-uploader-released/"/>
    <updated>2014-11-11T13:01:58-08:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/11/11/tn-s3-file-uploader-released</id>
    <content type="html"><![CDATA[<h2>Announcement</h2>

<p>Thinknear is delighted to announce that our <code>tn_s3_file_uploader</code> is released as open-source under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">APLv2</a>. <code>tn_s3_file_uploader</code> is a Ruby gem that we use internally to upload log files to Amazon S3 where they can be stored until we need to retrieve them for further analysis or processing.</p>

<p>You can visit the gem&rsquo;s page on <a href="https://rubygems.org/gems/tn_s3_file_uploader">Ruby Gems</a>. Please, visit the project&rsquo;s <a href="https://github.com/ThinkNear/tn_s3_file_uploader/wiki/Getting-started">getting-started page</a> for more information on how to install and use the gem.</p>

<h2>Motivation</h2>

<p>Thinknear&rsquo;s <a href="http://en.wikipedia.org/wiki/Real-time_bidding">Real Time Bidding</a> service produces billions of log entries per day and we have set up a simple but robust pipeline around rotating those logs and storing them on <a href="http://aws.amazon.com/s3/">Amazon S3</a>. The two key parts of this pipeline have been released to the public as open-source software.</p>

<p>First our <a href="http://aws.amazon.com/elasticbeanstalk/?sc_channel=PS&amp;sc_campaign=beanstalk_awns&amp;sc_publisher=google&amp;sc_medium=beanstalk_b&amp;sc_content=elastic_beanstalk_e&amp;sc_detail=amazon%20elastic%20beanstalk&amp;sc_category=beanstalk&amp;sc_segment=177&amp;sc_matchtype=e&amp;sc_country=US&amp;s_kwcid=AL!4422!3!50999240322!e!!g!!amazon%20elastic%20beanstalk&amp;ef_id=VCxP0QAABUt7hwuO:20141111211834:s">AWS Elastic Beanstalk</a> <a href="https://github.com/ThinkNear/aws_templates">logrotate and config scripts</a> rotates the logs produced by our service.</p>

<p>Second the aforementioned ruby gem uploads the rotated logs on S3 with a year/month/day/hour <a href="https://github.com/ThinkNear/tn_s3_file_uploader/wiki/Getting-started#partition-uploaded-files-based-on-date">partitioned destination path</a> that makes it easy to retrieve and analyze the logs for a specific period of time.</p>

<h2>Contribute</h2>

<p>Thinknear Engineering values the openness of the open source software model and we would like to invite anyone that finds <code>tn_s3_file_uploader</code> useful to contribute. Visit the <a href="https://github.com/ThinkNear/tn_s3_file_uploader/issues">project&rsquo;s issue tracker page</a> to submit questions, bugs or new features.</p>

<p>If you want to hack around the code, <a href="https://github.com/ThinkNear/tn_s3_file_uploader/fork">fork the project on GitHub</a> and see the <a href="https://github.com/ThinkNear/tn_s3_file_uploader/wiki/Contribute">how to contribute page</a> if you want to submit your code to the project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring Beanstalk With Collectd]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/10/21/monitoring-beanstalk-with-collectd/"/>
    <updated>2014-10-21T17:51:02-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/10/21/monitoring-beanstalk-with-collectd</id>
    <content type="html"><![CDATA[<h2>Our system</h2>

<p>Thinknear&rsquo;s <a href="http://en.wikipedia.org/wiki/Real-time_bidding">Real Time Bidding</a> service is built on <a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">AWS Elastic Beanstalk</a>.
We run around one-hundred instances to serve billions of auctions a day.
The system must be highly available, performant and maintain high success rates.
Operationally, engineers must be able to ship code throughout the week and change configurations without disrupting service.
We make data driven decisions to meet these standards by using metrics gathering systems like <a href="https://collectd.org/">collectd</a>.</p>

<blockquote><p>If you don&rsquo;t know where you are going, you&rsquo;ll end up someplace else &ndash; Yogi Berra</p></blockquote>

<p>If you run your services in the cloud, then you know there is a risk of overpaying.
There are a lot of factors that go into your cloud’s cost.
For example, we want to know when:</p>

<ul>
<li>We are under-utilizing instances.</li>
<li>Hosts are suffering from <a href="http://blog.scoutapp.com/articles/2013/07/25/understanding-cpu-steal-time-when-should-you-be-worried">steal time</a>.</li>
<li>We need a instance type that better suits our needs.</li>
</ul>


<p>Tracking memory, CPU and network statistics are key to meeting these goals and making sound decisions.
But we don&rsquo;t stop there.
There are multitudes of metrics to track.</p>

<!-- more -->


<h2>Measure everything</h2>

<p>At the instance level, AWS EC2 CloudWatch metrics from a Beanstalk environment didn&rsquo;t meet our needs.
CloudWatch provides CPU, Disk IO and Network IO; however, it lacks load, memory, fine-grained configurations and a method to extend the set of collected metrics.
Our engineering team needs more.
That is why we are users of the rock-solid statistics collection daemon collectd.</p>

<p>Collectd can track any system metric we want &ndash; the <a href="https://collectd.org/wiki/index.php/Table_of_Plugins">official plugins list</a> is extensive.
It works as a daemon running a loop over a set of plugins.
There is no graphing built into collectd so one must be configured.
<a href="http://graphite.readthedocs.org/en/1.0/tools.html">Graphite</a> is a popular frontend &ndash; but it only does graphing.
We like <a href="https://metrics.librato.com/">Librato</a>, a paid service, which has beautiful graphs, aggregates metrics and provides alerting similar to CloudWatch.</p>

<blockquote><p>Do not look where you fell, but where you slipped &ndash; African proverb</p></blockquote>

<p>We use collectd to track memory, load, cpu, network, JVM, process statistics and more.
Every Beanstalk host is deployed with a collectd installation as a standalone metric collector and reporter that is configured.
It is configured to report to Librato every minute.
This gives us the detailed performance tracking and a monitor for unexpected events.
When things go wrong, we can see what, when and how often an event happened.
When we make instance type changes, we can see exactly how we use system resources we need for each host.</p>

<h2>Getting started with Elastic Beanstalk and collectd</h2>

<p>Setting up collectd with Beanstalk is easy.
Below is a simple <a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html">Elastic Beanstalk configuration</a> that will:</p>

<ol>
<li>Install collectd on your instances.</li>
<li>Create a basic collectd configuration file for collecting CPU, network IO, load and memory statistics.
It uses the write_http plugin to report to Librato.</li>
<li>Start and supervise collectd daemon with the collectdmon monitor process.
There are alternatives like <a href="http://mmonit.com/monit/">monit</a>, but we like collectdmon because it is installed with the collectd package.</li>
</ol>


<p>This configuration was tested on a 2014.03 Amazon Linux EC2 instance.
This version of Amazon Linux comes with <a href="http://aws.amazon.com/amazon-linux-ami/2014.03-packages/">collectd-5.4.1</a> as an available RPM package.</p>

<p>Put this file into your project&rsquo;s <code>.ebextensions</code> directory.
Replace the <code>User</code> and <code>Password</code> fields with your credentials.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="c1"># .ebextensions/collectd.config</span>
</span><span class='line'><span class="l-Scalar-Plain">files</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="s">&quot;/etc/collectd.conf&quot;</span> <span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">mode</span><span class="p-Indicator">:</span> <span class="s">&quot;000664&quot;</span>
</span><span class='line'>    <span class="l-Scalar-Plain">owner</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">root</span>
</span><span class='line'>    <span class="l-Scalar-Plain">group</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">root</span>
</span><span class='line'>    <span class="l-Scalar-Plain">content</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
</span><span class='line'>      <span class="no">LoadPlugin syslog</span>
</span><span class='line'>      <span class="no">LoadPlugin cpu</span>
</span><span class='line'>      <span class="no">LoadPlugin interface</span>
</span><span class='line'>      <span class="no">LoadPlugin load</span>
</span><span class='line'>      <span class="no">LoadPlugin memory</span>
</span><span class='line'>      <span class="no">LoadPlugin write_http</span>
</span><span class='line'>      <span class="no">&lt;Plugin write_http&gt;</span>
</span><span class='line'>        <span class="no">&lt;URL &quot;https://collectd.librato.com/v1/measurements&quot;&gt;</span>
</span><span class='line'>          <span class="no">User &quot;user@example.com&quot;</span>
</span><span class='line'>          <span class="no">Password &quot;password&quot;</span>
</span><span class='line'>          <span class="no">Format &quot;JSON&quot;</span>
</span><span class='line'>        <span class="no">&lt;/URL&gt;</span>
</span><span class='line'>      <span class="no">&lt;/Plugin&gt;</span>
</span><span class='line'>      <span class="no">Include &quot;/etc/collectd.d&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">packages</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">yum</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">collectd</span><span class="p-Indicator">:</span> <span class="p-Indicator">[]</span>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">commands</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">start_collectd</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">command</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">killall collectd; collectdmon -c collectd</span>
</span></code></pre></td></tr></table></div></figure>


<p>Collectd will start reporting metrics to Librato when you deploy your application.</p>

<p>Then you can start building Librato instruments and dashboards that look like this:</p>

<p><img src="http://engineering.thinknear.com/images/collectd_cpu.png" alt="collectd CPU" /></p>

<p><img src="http://engineering.thinknear.com/images/collectd_dash.png" alt="collectd dash" /></p>

<h2>Summary</h2>

<p>At Thinknear, we strive to measure anything that will help us understand the behavior of our systems in production.
Collectd is a great tool for us to track instance level statistics that CloudWatch does not provide.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unnecessary Complexity]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/08/22/unnecessary-complexity/"/>
    <updated>2014-08-22T17:20:18-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/08/22/unnecessary-complexity</id>
    <content type="html"><![CDATA[<div align="center">
&#8220;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius — and a lot of courage — to move in the opposite direction.&#8221; - Einstein
</div>


<p><p/>
Software development is hard.  That&rsquo;s a fact.  Not only do we have a product that we must enhance, develop from scratch,
or even maintain, but we have people&rsquo;s interactions, opinions, emotions, the list goes on.  And then there is design,
technology and execution.   All these factors, all these variables make software development the fantastic challenge
that it is.  And I love it.  At the same time, I am on a mission to remove complexity introduced into a system when it
need not be there.  The actual doing of this, making the decisions about what to include, what to leave out and to do
this consistently on a day to day basis is difficult.</p>

<p>As Product Owners and Engineers, we are faced with these decisions all the time as we work on a system.  Each decision,
from those more-complex-than-it-needs-to-be features to irreversible design decisions to that single line of code, all
have an impact on our productivity and agility.  Everybody talks about keeping it simple and taking a lean approach -
this is nothing new.  It is how we can achieve this when making those decisions on the ground that is the focus of this
blog.</p>

<p><!-- more --></p>

<p>There are at least five major areas where we love to introduce complexity: feature development, execution, design,
technology selection and code.</p>

<p>Here I want to go through a number of approaches that I have used that help prevent complexity being incurred because,
let&rsquo;s face it, the solution should only be as complex as the problem at hand.</p>

<h2>Unnecessary feature complexity</h2>

<p>The basic tenet of unnecessary feature complexity, especially when dealing with new product development,
is to answer the question &ldquo;What is the minimum we have to do to figure out if this is going to work?&rdquo; Again, nothing
new.  This is part of the Lean movement.  I&rsquo;m sure you have your own, but allow me to to offer the following approach
to exploring a new product or service:</p>

<ul>
<li>What is the minimum amount of work we need to do in order to have the initial conversation with the first client?

<ul>
<li>Rough idea on features available</li>
<li>Rough idea on tech, format, deployment and performance.  Discussion based, no documentation required.</li>
</ul>
</li>
<li>What is the minimum amount of work we need to do to sell to our first client?

<ul>
<li>Minimum Marketable Product (MMP) determined along with feasibility</li>
<li>More information about data required to be supplied and what would be returned.</li>
<li>NOT the API contract</li>
<li>(Internal only) Rough idea of implementation time to version 0.1 for first client.</li>
</ul>
</li>
<li>Once we have sold with delivery estimate to client:

<ul>
<li>Lock down API contract or feature set including success/failure scenarios - deliver early so client can start mocking
out from their end and write the integration code</li>
<li>Integration approach detailed</li>
<li>Deployment architecture detailed</li>
<li>Performance characteristics and design to support detailed</li>
<li>Implement and deliver MMP</li>
<li>Roadmap for any further features created based on initial usage (iterative and incremental)</li>
</ul>
</li>
</ul>


<p>Pretty straight forward, huh?  Yet we all know when we have done too much design after a product or idea has been put
on the backburner.  Ever thought &ldquo;We really didn&rsquo;t need to do that much work to make that decision to defer the product&rdquo;? Make the hard decisions now to save time later.</p>

<p>Of course, there are times when the above steps do not make sense, or they need to be modified.  For example,
when doing a legacy migration. There are likely many clients that rely on features already present in the legacy system. In these cases,
we can take the opportunity to evolve the clients during the migration and do some spring cleaning of
the legacy system, i.e. answer the questions &ldquo;What features do we really need?&rdquo; and &ldquo;What features have proven to not
be of value given what we know now about our business?&rdquo;  Unfortunately, as is most often the case, there are &ldquo;hidden&rdquo;
organic features that have no documentation that can trip us up.</p>

<h2>Unnecessary execution complexity</h2>

<p>There are many in this category, so let&rsquo;s just focus on a few:</p>

<ul>
<li>Break big problems down into smaller pieces:  Be exact with the acceptance criteria.  This one is self explanatory.</li>
<li>Business Accepting a story only when it is deployed and verified in production: This is a technique that <a href="http://mikquinlan.com/2013/07/28/no-more-burndown-no-more-definition-of-done/" target="_blank">I&rsquo;ve spoken about before.</a>
It encourages quality and promotes continuous deployment, a part of the Continuous Delivery strategy.</li>
<li>Keep to 1 week iterations if suitable: As long as there is a long term strategy, <a href="http://mikquinlan.com/2013/03/07/re-platforming-a-system-and-the-value-of-1-week-iterations/" target="_blank">1 week iterations</a>
give more opportunities to retrospect and force breaking down of stories into smaller component parts of business value while keeping the overall mission squarely in the centre of the picture.</li>
<li>Automate delivery:  Once we have small stories and iterative, incremental development, deploying manually,
especially if we are practicing BA&#8217;ing a story once it is in production, takes a lot of effort. Make the investment in
Continuous Delivery supported by automation, taking into account the aspects <a href="http://mikquinlan.com/2014/01/27/continuous-delivery-the-missing-piece-of-the-puzzle/" target="_blank">I have blogged about before</a>.
Micro Services are also a viable option (note: Micro Services is a bad name, IMO, a service should just be small enough to fit in our
headspace&hellip;but that is a topic for another blog).</li>
</ul>


<h2>Unnecessary design complexity</h2>

<p>Keep it simple. We all know the acronym. Here are some ideas to help focus development efforts:</p>

<ul>
<li>Limit the number of business strategies/initiatives an organisation takes on:  This could actually be a section all
on its own.  I have frequently been in organisations where multiple business strategies are pursued concurrently.<br/>
There is nothing wrong with pursuing multiple business strategies, but when it exhausts the capacity of the organisation,
then we end up with a lot of half done ideas that never gain the momentum they&rsquo;re supposed to due to not enough attention
being paid to them (market forces notwithstanding).  This then trickles down into design.  Design is hard.  We must
make sure we design within our capacity so we build something that will work and work well enough to fulfill the most
valuable strategic objectives that we carefully chose.</li>
<li>Keep designs incredibly straight forward wherever possible: Again, not a new concept, yet when on the ground how many
times have you looked at a design and thought &ldquo;that is waaaay too complex&rdquo;?  More than once, maybe?  Not every edge
case needs to be covered.  Work with the business to understand the acceptable levels of failure a system can have.<br/>
During design, when an edge case comes up, if it&rsquo;s technical weigh up the likelihood of occurrence vs the effort to fix.
Don&rsquo;t just dive in and fix it because it&rsquo;s there.  If it&rsquo;s a business edge case, work with the business to see if it
needs to be resolved or whether a failure is acceptable.  Again, make hard decisions here.  Overall, taking a little
longer to decide that a feature will not be implemented will save effort over a quick decision to do it then having
the entire team take on that complexity.</li>
</ul>


<h2>Unnecessary technology complexity</h2>

<p>Keep the learning curve as low as possible so that engineers have less to learn technologically and can focus on the
solutions implemented to create the product or service.  Whatever we can do to have them focus more on the problems
being solved rather than the different libraries we used to, say, create and access files can only be a good thing:</p>

<ul>
<li>Organisationally it is important to limit the technologies being used so we can build upon the knowledge acquired by
our fellow engineers.  If we have one NoSQL database, do we really need another?  For a technology selection where
NoSQL is deemed to be a good fit, it is important to come up with a reason why we cannot use a NoSQL technology that
is already in production within our organisation, along with all the libraries we have built around it.  Of course,
if that technology is not a fit, we then must be able to articulate the distinct advantages of the technology we want
to introduce.  Polyglot solutions are a good thing, applied judiciously.  Again, effort vs value.  In this case, effort
to de-risk, implement and build libraries/tools around the new technology vs value to the business.</li>
<li>Within a service it is important to limit the libraries used.  The number of libraries a single service can end up
using can be huge.  That huge effective pom.xml if you use Maven, that long list of gems.  Limit the libraries used for
a service so that engineers will not just reach for the library they know when facing a problem, adding to the burgeoning
list of libraries required for the project.</li>
</ul>


<h2>Unnecessary code/implementation complexity</h2>

<p>Have you ever looked at a codebase and seen multiple ways of achieving the same thing?  For example, file handling,
filtering specific objects out of lists etc.?  Yeah, me too.  It&rsquo;s important that we do common operations consistently
within a codebase (and across services&hellip;harder but still possible).  Agree within the team how this should be done
(use the <a href="https://code.google.com/p/guava-libraries/" target="_blank">Guava libraries</a>? <a href="http://commons.apache.org/" target="_blank">Apache Commons</a>? etc.)
and stick to it.  If you are an engineer and unsure, look at other parts of the codebase.  Common methods of handling
code results in less to learn and a decreased ramp up period for engineers new to it.</p>

<h2>A note on back end infrastructure systems vs front end product development</h2>

<p>Front end product development needs rapid delivery.  So we can&rsquo;t always live in a world where everything is consistent.  In these scenarios, agree among the team members the libraries to use, approach etc. and forge ahead developing the new product.  In this case, technical debt will ensue, but this is good technical debt.  In fact, we could say it is &ldquo;necessary&rdquo; technical debt. Necessary because not adding a feature can be as key to being first to market as adding a feature.</p>

<p>If the product or feature is successful and is moved from proof to long lived, address the technical debt that has been incurred.  This typically happens when a product gets traction and a company is coming out of the startup phase. A great blog that articulates technical debt is
<a href="http://blog.crisp.se/2013/10/11/henrikkniberg/good-and-bad-technical-debt" target="_blank">this one</a> by Henrik Kniberg.</p>

<h2>Summary</h2>

<p>I hope the above guidance helps.  Each of the bullet points could be a topic in itself, and I wanted this blog to be a collection of areas to consider.  If I was to sum this up in one sentence, it would be</p>

<p>&ldquo;Take just a little more time to make the hard decisions.&rdquo;</p>

<p>I am always looking for ways to make the development of software and, by extension, products easier, so I&rsquo;d be interested to know what strategies you use to limit complexity in a system. Feel free to leave a comment, below.</p>

<p>Aaaaand as I completed this, that lovely chap Dan North just released <a href="https://www.youtube.com/watch?v=XqgwHXsQA1g" target="_blank">this presentation</a>. :-)</p>

<p>Note: This is a modified cross post from one that originally appeared <a href="http://mikquinlan.com/2014/08/14/unnecessary-complexity/" target="_blank">here</a>.</p>

<p><strong>About the author</strong>: Mik Quinlan is Director of Engineering - Mobile Advertising at Thinknear by Telenav. He has
over 16 years experience architecting and implementing highly scalable, high throughput, low latency systems and
has led several Agile transformations at various companies. Connect via <a href="https://www.linkedin.com/in/mikquinlan" target="_blank">LinkedIn</a>
or <a href="https://twitter.com/mikquinlan" target="_blank">Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Welcome to Thinknear Engineering]]></title>
    <link href="http://engineering.thinknear.com/blog/2014/08/20/welcome-to-thinknear-engineering/"/>
    <updated>2014-08-20T17:53:07-07:00</updated>
    <id>http://engineering.thinknear.com/blog/2014/08/20/welcome-to-thinknear-engineering</id>
    <content type="html"><![CDATA[<p>This blog marks the beginning of the Thinknear Engineering presence on the internet.
<a href="http://thinknear.com/">Thinknear</a> started in 2011 and was acquired by <a href="http://www.telenav.com/">Telenav</a> in 2012.
Though we&rsquo;re part of a much larger public company, Thinknear maintains its distinct engineering culture and spirit.
We have been entirely cloud-based from the get-go (we&rsquo;re big users of AWS, Heroku, Librato, and more); we&rsquo;re very agile,
using a scrum-based internal development cycle; and we have a lot of fun, like with daily Halo breaks.</p>

<p>We also push the limits of what a small team can accomplish with today&rsquo;s technology.
We handle tens of thousands of requests per second across our fleet with very aggressive latency targets,
our hosts generate terabytes of data per day which we have to crunch with aggressive time goals,
we&rsquo;re running massive PostGIS queries, and we dabble in Angular.js as well.</p>

<p>On this blog, we will showcase our technology, expertise, open source contributions, and other things of interest to
distributed systems&#8217; engineers. We are really excited about what we do, and we hope to share our enthusiasm here.</p>
]]></content>
  </entry>
  
</feed>
